{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-03T06:46:59.100955Z","iopub.execute_input":"2021-11-03T06:46:59.101616Z","iopub.status.idle":"2021-11-03T06:46:59.140049Z","shell.execute_reply.started":"2021-11-03T06:46:59.101511Z","shell.execute_reply":"2021-11-03T06:46:59.139424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load packages & data\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# SimpleImputer replaces the previous sklearn.preprocessing.Imputer estimator which is now removed.\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('display.max_columns', 100)\n\ntrain=pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv')\ntest=pd.read_csv('../input/porto-seguro-safe-driver-prediction/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:46:59.141887Z","iopub.execute_input":"2021-11-03T06:46:59.143309Z","iopub.status.idle":"2021-11-03T06:47:11.486515Z","shell.execute_reply.started":"2021-11-03T06:46:59.143263Z","shell.execute_reply":"2021-11-03T06:47:11.485553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Data overall","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:47:11.487828Z","iopub.execute_input":"2021-11-03T06:47:11.488434Z","iopub.status.idle":"2021-11-03T06:47:11.53599Z","shell.execute_reply.started":"2021-11-03T06:47:11.488397Z","shell.execute_reply":"2021-11-03T06:47:11.535094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.tail()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:47:11.538113Z","iopub.execute_input":"2021-11-03T06:47:11.538373Z","iopub.status.idle":"2021-11-03T06:47:11.578002Z","shell.execute_reply.started":"2021-11-03T06:47:11.538342Z","shell.execute_reply":"2021-11-03T06:47:11.577018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-binary variables\n\n-categorical variables (integer)\n\n-integer / float variables\n\n* -1 은 missing value를 의미한다","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:47:11.579073Z","iopub.execute_input":"2021-11-03T06:47:11.579338Z","iopub.status.idle":"2021-11-03T06:47:11.586569Z","shell.execute_reply.started":"2021-11-03T06:47:11.579311Z","shell.execute_reply":"2021-11-03T06:47:11.585512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop_duplicates()\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:47:11.587832Z","iopub.execute_input":"2021-11-03T06:47:11.588401Z","iopub.status.idle":"2021-11-03T06:47:12.588999Z","shell.execute_reply.started":"2021-11-03T06:47:11.588349Z","shell.execute_reply":"2021-11-03T06:47:12.58817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:47:12.590308Z","iopub.execute_input":"2021-11-03T06:47:12.590623Z","iopub.status.idle":"2021-11-03T06:47:12.673816Z","shell.execute_reply.started":"2021-11-03T06:47:12.590592Z","shell.execute_reply":"2021-11-03T06:47:12.672895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Metadata\n\n    column 에 대해서 metadata를 따로 저장\n\n        role (ID, target, input)\n\n        level (binary, nominal, interval, ordinal)\n\n        keep (True, False)\n\n        dtype (int, float, str)","metadata":{}},{"cell_type":"code","source":"data = []\nfor c in train.columns:\n    # role\n    if c == 'id':\n        role = 'id'\n    elif c == 'target':\n        role = 'target'\n    else :\n        role = 'input'\n        \n    # level\n    if 'bin' in c or c == 'target':\n        level = 'binary'\n    elif 'cat' in c or c == 'id':\n        level = 'nominal'\n    elif train[c].dtype == float:\n        level = 'interval'\n    elif train[c].dtype == int :\n        level = 'ordinal'\n        \n    # keep\n    keep = True\n    if c == 'id':\n        keep = False\n    \n    # dtype\n    dtype = train[c].dtype\n    \n    c_dict = {\n        'varname' : c,\n        'role' : role,\n        'level' : level,\n        'keep' : keep,\n        'dtype' : dtype\n    }\n    data.append(c_dict)\n    \nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:47:12.675281Z","iopub.execute_input":"2021-11-03T06:47:12.675522Z","iopub.status.idle":"2021-11-03T06:47:12.686825Z","shell.execute_reply.started":"2021-11-03T06:47:12.675494Z","shell.execute_reply":"2021-11-03T06:47:12.686047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:47:12.68819Z","iopub.execute_input":"2021-11-03T06:47:12.688404Z","iopub.status.idle":"2021-11-03T06:47:12.717324Z","shell.execute_reply.started":"2021-11-03T06:47:12.688379Z","shell.execute_reply":"2021-11-03T06:47:12.716653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta[(meta.level == 'nominal')&(meta.keep)].index","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:47:12.720902Z","iopub.execute_input":"2021-11-03T06:47:12.721179Z","iopub.status.idle":"2021-11-03T06:47:12.73578Z","shell.execute_reply.started":"2021-11-03T06:47:12.721149Z","shell.execute_reply":"2021-11-03T06:47:12.734829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Descriptive statistics","metadata":{}},{"cell_type":"code","source":"x = meta[(meta.level=='interval') & (meta.keep)].index\ntrain[x].describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:47:12.737352Z","iopub.execute_input":"2021-11-03T06:47:12.738112Z","iopub.status.idle":"2021-11-03T06:47:13.032935Z","shell.execute_reply.started":"2021-11-03T06:47:12.73806Z","shell.execute_reply":"2021-11-03T06:47:13.032069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"missing value가 존재하는 column (min이 -1) 의 경우 그렇지 않은 column들과\n\n범위, 평균 등에서 차이가 난다 -> scaling 필요!","metadata":{}},{"cell_type":"code","source":"x = meta[(meta.level=='ordinal') & (meta.keep)].index\ntrain[x].describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:47:13.034044Z","iopub.execute_input":"2021-11-03T06:47:13.034262Z","iopub.status.idle":"2021-11-03T06:47:13.396674Z","shell.execute_reply.started":"2021-11-03T06:47:13.034237Z","shell.execute_reply":"2021-11-03T06:47:13.395796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = meta[(meta.level == 'binary') & (meta.keep)].index\ntrain[v].describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:47:13.398173Z","iopub.execute_input":"2021-11-03T06:47:13.399677Z","iopub.status.idle":"2021-11-03T06:47:13.739559Z","shell.execute_reply.started":"2021-11-03T06:47:13.399621Z","shell.execute_reply":"2021-11-03T06:47:13.738708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"target의 mean이 0.0365인 것으로 보아, target value가 불균형적임을 알 수 있다.\n\n(target=0인 record가 target=1인 record보다 훨씬 많음)\n\n4. Handling Imbalanced classes\n\n    - target=1 인 record를 oversampling\n    \n    - target=0 인 record undersampling\n    \n    - 등등...","metadata":{}},{"cell_type":"code","source":"# large training set -> undersampling 사용\n\ndesired_apriori = 0.10\n\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target=0\nundersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# Randomly select records with target=0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrain = train.loc[idx_list].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:47:13.740821Z","iopub.execute_input":"2021-11-03T06:47:13.741053Z","iopub.status.idle":"2021-11-03T06:47:14.326759Z","shell.execute_reply.started":"2021-11-03T06:47:13.741025Z","shell.execute_reply":"2021-11-03T06:47:14.325871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. Data quality Checks\n\n    5.1 Check missing values\n    \n    5.2 Check cardinality of categorical variables\n    \n        cardinality = number of different values in a variable\n        \n        categorical column은 dummy variable로 바꿀 예정\n        \n        check whether there are many distinct values, as they would result in many dummy variables.","metadata":{}},{"cell_type":"code","source":"cols_with_missing = []\n\nfor c in train.columns:\n    missings = train[train[c]==-1][c].count()\n    if missings > 0:\n        cols_with_missing.append(c)\n        missings_perc = missings / train.shape[0]\n        \n        print('Column {} has {} records ({:.2%}) with missing values'.format(c, missings, missings_perc))\n        \nprint('In total, {} columns with missing values'.format(len(cols_with_missing)))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:47:14.328013Z","iopub.execute_input":"2021-11-03T06:47:14.328293Z","iopub.status.idle":"2021-11-03T06:47:14.46919Z","shell.execute_reply.started":"2021-11-03T06:47:14.328264Z","shell.execute_reply":"2021-11-03T06:47:14.468263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ps_car_03_cat, ps_car_05_cat은 missing value의 비율이 크므로 열 제거\n\n그 외의 경우, -1을 mean, mode 등으로 대체하는 방법을 사용할 수 있다.","metadata":{}},{"cell_type":"code","source":"cols_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\n# train.drop(cols_to_drop, inplace=True, axis=1)\nmeta.loc[cols_to_drop, 'keep'] = False\n\n# Imputing with the mean or mode\nmean_imp = SimpleImputer(missing_values=-1, strategy='mean')\nmode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:51:10.229299Z","iopub.execute_input":"2021-11-03T06:51:10.229644Z","iopub.status.idle":"2021-11-03T06:51:10.277288Z","shell.execute_reply.started":"2021-11-03T06:51:10.229607Z","shell.execute_reply":"2021-11-03T06:51:10.276133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = meta[(meta.level=='nominal')&(meta.keep)].index\nfor c in v:\n    dist_values = train[c].value_counts().shape[0]\n    print('Column {} has {} distinct values'.format(c, dist_values))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:00:09.354405Z","iopub.execute_input":"2021-11-03T07:00:09.35474Z","iopub.status.idle":"2021-11-03T07:00:09.386985Z","shell.execute_reply.started":"2021-11-03T07:00:09.354705Z","shell.execute_reply":"2021-11-03T07:00:09.385288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Script by https://www.kaggle.com/ogrellier\n# Code: https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)\ntrain_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n                             test[\"ps_car_11_cat\"], \n                             target=train.target, \n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)\n    \ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat','keep'] = False  # Updating the meta\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:14:58.870062Z","iopub.execute_input":"2021-11-03T07:14:58.870378Z","iopub.status.idle":"2021-11-03T07:14:59.383965Z","shell.execute_reply.started":"2021-11-03T07:14:58.870347Z","shell.execute_reply":"2021-11-03T07:14:59.38305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6. EDA","metadata":{}},{"cell_type":"code","source":"# Categorical variables\n\nv = meta[(meta.level == 'nominal')&(meta.keep)].index\nfor c in v:\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(20,10))\n    \n    # calculate the percentage of target=1 per category\n    cat_perc = train[[c, 'target']].groupby([c], as_index=False).mean()\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    \n    # Bar plot\n    sns.barplot(ax=ax, x=c, y='target', data=cat_perc, order=cat_perc[c])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(c, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:15:02.372429Z","iopub.execute_input":"2021-11-03T07:15:02.37274Z","iopub.status.idle":"2021-11-03T07:15:05.382206Z","shell.execute_reply.started":"2021-11-03T07:15:02.372709Z","shell.execute_reply":"2021-11-03T07:15:05.381081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"몇 categorical column에서, missing value의 row에서 보험 청구 확률 (target=0의 비율)이 높게 나타난다\n\n->  It is a good idea to keep the missing values as a separate category value, instead of replacing them by the mode for instance","metadata":{}},{"cell_type":"code","source":"# Correlations between Interval variables\n\ndef corr_heatmap(v):\n    correlations = train[v].corr()\n    \n    # Create color map ranging between two colors\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    plt.show();\n    \nv = meta[(meta.level == 'interval') & (meta.keep)].index\ncorr_heatmap(v)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:19:48.810422Z","iopub.execute_input":"2021-11-03T07:19:48.810761Z","iopub.status.idle":"2021-11-03T07:19:49.855442Z","shell.execute_reply.started":"2021-11-03T07:19:48.810728Z","shell.execute_reply":"2021-11-03T07:19:49.854379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a strong correlations between the variables:\n\n    ps_reg_02 and ps_reg_03 (0.7)\n\n    ps_car_12 and ps_car13 (0.67)\n\n    ps_car_12 and ps_car14 (0.58)\n\n    ps_car_13 and ps_car15 (0.67)","metadata":{}},{"cell_type":"code","source":"s = train.sample(frac=0.1)\n# train data의 일부만 sampling해서 correlation 살펴보기","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:24:58.946921Z","iopub.execute_input":"2021-11-03T07:24:58.947392Z","iopub.status.idle":"2021-11-03T07:24:58.974288Z","shell.execute_reply.started":"2021-11-03T07:24:58.947356Z","shell.execute_reply":"2021-11-03T07:24:58.97344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:25:10.708371Z","iopub.execute_input":"2021-11-03T07:25:10.709041Z","iopub.status.idle":"2021-11-03T07:25:12.62075Z","shell.execute_reply.started":"2021-11-03T07:25:10.708998Z","shell.execute_reply":"2021-11-03T07:25:12.619591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:25:23.603627Z","iopub.execute_input":"2021-11-03T07:25:23.60432Z","iopub.status.idle":"2021-11-03T07:25:25.310614Z","shell.execute_reply.started":"2021-11-03T07:25:23.60428Z","shell.execute_reply":"2021-11-03T07:25:25.30959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:25:59.759859Z","iopub.execute_input":"2021-11-03T07:25:59.760185Z","iopub.status.idle":"2021-11-03T07:26:01.548447Z","shell.execute_reply.started":"2021-11-03T07:25:59.760137Z","shell.execute_reply":"2021-11-03T07:26:01.547612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:26:55.386041Z","iopub.execute_input":"2021-11-03T07:26:55.386366Z","iopub.status.idle":"2021-11-03T07:26:57.146672Z","shell.execute_reply.started":"2021-11-03T07:26:55.386335Z","shell.execute_reply":"2021-11-03T07:26:57.145558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation between ordinal variables\n\nv = meta[(meta.level == 'ordinal')&(meta.keep)].index\ncorr_heatmap(v)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:33:20.492628Z","iopub.execute_input":"2021-11-03T07:33:20.492969Z","iopub.status.idle":"2021-11-03T07:33:22.185261Z","shell.execute_reply.started":"2021-11-03T07:33:20.492938Z","shell.execute_reply":"2021-11-03T07:33:22.184205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"7. Feature Engineering\n\n    - Create dummy variables\n    - Create interaction variables","metadata":{}},{"cell_type":"code","source":"v = meta[(meta.level == 'nominal')&(meta.keep)].index\nprint('Before dummification : {} variables'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('After dummification : {} variables'.format(train.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:40:17.131706Z","iopub.execute_input":"2021-11-03T07:40:17.132611Z","iopub.status.idle":"2021-11-03T07:40:17.298152Z","shell.execute_reply.started":"2021-11-03T07:40:17.132563Z","shell.execute_reply":"2021-11-03T07:40:17.297006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = meta[(meta.level == 'interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)  # Remove the original columns\n# Concat the interaction variables to the train data\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:42:17.959389Z","iopub.execute_input":"2021-11-03T07:42:17.960352Z","iopub.status.idle":"2021-11-03T07:42:18.406885Z","shell.execute_reply.started":"2021-11-03T07:42:17.960304Z","shell.execute_reply":"2021-11-03T07:42:18.405949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"8. Feature selection\n\n    - Removing features with zero or low variance\n      \n      sklearn \"VarianceThreshold\"","metadata":{}},{"cell_type":"code","source":"selector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop(['id','target'], axis=1))\n\nf = np.vectorize(lambda x : not x)\n\nv = train.drop(['id', 'target'], axis=1).columns[f(selector.get_support())]\nprint('{} variables have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T08:13:58.668846Z","iopub.execute_input":"2021-11-03T08:13:58.66917Z","iopub.status.idle":"2021-11-03T08:13:59.400487Z","shell.execute_reply.started":"2021-11-03T08:13:58.669134Z","shell.execute_reply":"2021-11-03T08:13:59.399271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Selecting features with a Random Forest and sklearn \"SelectFromModel\"","metadata":{}},{"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nfeat_labels = X_train.columns\n\n# Use feature importance of a random forest\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n\nindices = np.argsort(rf.feature_importances_)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]]))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T08:14:03.580017Z","iopub.execute_input":"2021-11-03T08:14:03.580775Z","iopub.status.idle":"2021-11-03T08:15:10.760388Z","shell.execute_reply.started":"2021-11-03T08:14:03.580732Z","shell.execute_reply":"2021-11-03T08:15:10.759071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set threshold on the level of feature importance\nsfm = SelectFromModel(rf, threshold='median', prefit=True) # top 50% 선택\nprint('Before selection: {} features'.format(X_train.shape[1]))\nn_features = sfm.transform(X_train).shape[1]\nprint('After selection: {} features'.format(n_features))\nselected_vars = list(feat_labels[sfm.get_support()])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"9. Feature scaling","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit_transform(train.drop(['target'], axis=1))","metadata":{},"execution_count":null,"outputs":[]}]}